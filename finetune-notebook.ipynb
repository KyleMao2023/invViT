{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4090f386",
   "metadata": {},
   "source": [
    "## Stage 1: Fine-tune a Vision Transformer (ViT) Model\n",
    "\n",
    "### Step 1: Import libraries and define constants\n",
    "\n",
    "- Replace `PRETRAINED_PATH` with the path to your pre-trained ViT model.\n",
    "\n",
    "- Replace `IMG_DIR` with the path to your ImgCeleba dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc42fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import ViTForImageClassification, ViTImageProcessor, ViTConfig, ViTModel\n",
    "from skimage.metrics import mean_squared_error, peak_signal_noise_ratio, structural_similarity\n",
    "\n",
    "\n",
    "IMG_DIR = './TransformInverse/ImgCeleba/img_celeba'\n",
    "PRETRAINED_PATH = \"./TransformInverse\"\n",
    "INVERSE_VIT_PATH = 'TransformInverse/model_0414c(Blackbox-Self).pth'\n",
    "NUM_LABELS = 40\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE_VIT = 3e-5\n",
    "LEARNING_RATE_CLASSIFIER = 1e-4\n",
    "NUM_EPOCHS = 5\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e40424",
   "metadata": {},
   "source": [
    "### Step 2: Adjust hyperparameters for fine-tuning\n",
    "\n",
    "- Label smoothing contributes to the robustness of defending MIA when the factor is **negative**.\n",
    "\n",
    "    $$\\mathcal{L}^{LS}(\\bold{y}, \\bold{p}, \\alpha) = (1-\\alpha) \\cdot \\mathcal{L}_{CE}(\\bold{y}, \\bold{p}) + \\frac{\\alpha}{C} \\cdot \\mathcal{L}_{CE} (\\bold{1}, \\bold{p})$$\n",
    "\n",
    "    > Be Careful What You Smooth For: Label Smoothing Can Be a Privacy Shield but Also a Catalyst for Model Inversion Attacks\n",
    "\n",
    "- The other methods do not make much sense when `RESET_ALL_PARAMETERS` is set to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d435549d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENABLE_LABEL_SMOOTHING = False\n",
    "LABEL_SMOOTHING_FACTOR = -0.1\n",
    "\n",
    "ENABLE_L1_REGULARIZATION = False\n",
    "L1_LAMBDA = 5e-6\n",
    "\n",
    "ENABLE_L2_REGULARIZATION = False\n",
    "weight_decay = 1e-4\n",
    "\n",
    "ENABLE_VIB = True\n",
    "beta = 1e-5\n",
    "\n",
    "RESET_ALL_PARAMETERS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df13759",
   "metadata": {},
   "source": [
    "### Step 3: Enhance original ViT with Information Bottleneck (if used)\n",
    "\n",
    "Classifier replaced with a linear layer outputing 40 classes as a downstream task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d8279f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTVIBForImageClassification(nn.Module):\n",
    "    def __init__(self, pretrained_model_name_or_path, num_labels, beta=1e-3, latent_dim_extension_factor=1):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "        self.num_labels = num_labels\n",
    "        try:\n",
    "            self.vit_backbone = ViTModel.from_pretrained(pretrained_model_name_or_path)\n",
    "            print(f\"Successfully loaded ViTModel from {pretrained_model_name_or_path}\")\n",
    "        except OSError:\n",
    "            print(f\"Could not load ViTModel directly. Attempting to load ViTForImageClassification and extract backbone from {pretrained_model_name_or_path}...\")\n",
    "            temp_full_model = ViTForImageClassification.from_pretrained(pretrained_model_name_or_path)\n",
    "            self.vit_backbone = temp_full_model.vit\n",
    "            print(\"Successfully extracted ViTModel (vit backbone).\")\n",
    "\n",
    "\n",
    "        self.config = self.vit_backbone.config\n",
    "        self.hidden_size = self.config.hidden_size\n",
    "\n",
    "        self.fc_mu = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.fc_logvar = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "\n",
    "        self.classifier = nn.Linear(self.hidden_size, num_labels)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.fc_mu.weight)\n",
    "        nn.init.zeros_(self.fc_mu.bias)\n",
    "        nn.init.xavier_uniform_(self.fc_logvar.weight)\n",
    "        nn.init.zeros_(self.fc_logvar.bias)\n",
    "        nn.init.xavier_uniform_(self.classifier.weight)\n",
    "        nn.init.zeros_(self.classifier.bias)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, pixel_values, labels=None):\n",
    "        outputs = self.vit_backbone(pixel_values=pixel_values)\n",
    "        last_hidden_state = outputs.last_hidden_state # (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        mu = self.fc_mu(last_hidden_state)         # (batch_size, seq_len, hidden_size)\n",
    "        logvar = self.fc_logvar(last_hidden_state) # (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        z = self.reparameterize(mu, logvar)      # (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        cls_token_representation = z[:, 0]      # (batch_size, hidden_size)\n",
    "        logits = self.classifier(cls_token_representation) # (batch_size, num_labels)\n",
    "\n",
    "        loss = None\n",
    "        kld_loss_total = None\n",
    "        if labels is not None:\n",
    "            classification_loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "            kld_loss_element_wise = 0.5 * (mu.pow(2) + logvar.exp() - 1 - logvar)\n",
    "\n",
    "            kld_loss_per_sample = torch.sum(kld_loss_element_wise, dim=[1, 2]) # Sum over seq_len and hidden_size\n",
    "            kld_loss_total = torch.mean(kld_loss_per_sample) # Average over batch\n",
    "\n",
    "            loss = classification_loss + self.beta * kld_loss_total\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"logits\": logits,\n",
    "            \"mu\": mu,\n",
    "            \"logvar\": logvar,\n",
    "            \"z_sampled\": z,\n",
    "            \"classification_loss\": classification_loss if labels is not None else None,\n",
    "            \"kld_loss\": kld_loss_total\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620e3123",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    processor = ViTImageProcessor.from_pretrained(PRETRAINED_PATH)\n",
    "except OSError:\n",
    "    print(\n",
    "        f\"Warning: Could not load ViTImageProcessor from {PRETRAINED_PATH}. \"\n",
    "        \"Attempting to load a default ViT processor. Make sure this is intended.\"\n",
    "    )\n",
    "    processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((processor.size[\"height\"], processor.size[\"width\"])),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=processor.image_mean, std=processor.image_std),\n",
    "    ]\n",
    ")\n",
    "\n",
    "vit_model = (\n",
    "    ViTVIBForImageClassification(PRETRAINED_PATH, num_labels=40, beta=beta)\n",
    "    if ENABLE_VIB\n",
    "    else ViTForImageClassification.from_pretrained(PRETRAINED_PATH)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2637f8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./TransformInverse/ImgCeleba/labels.pkl', \"rb\") as f:\n",
    "    labels = pickle.load(f)\n",
    "labels.keys()\n",
    "\n",
    "attr_df = labels['list_attr_celebA'].copy()\n",
    "attr_df = attr_df.replace(-1, 0)\n",
    "attr_labels = torch.tensor(attr_df.values, dtype=torch.float32)\n",
    "\n",
    "partition_df = labels['list_eval_partition']\n",
    "partition = partition_df[1].values\n",
    "img_filenames = attr_df.index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c990f457",
   "metadata": {},
   "source": [
    "### Step 3: Prepare ImgCeleba dataset and the optimizer with regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9183ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CelebADataset(Dataset):\n",
    "    def __init__(self, filenames, labels, img_dir, transform):\n",
    "        self.filenames = filenames\n",
    "        self.labels = labels\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.filenames[idx])\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            image = self.transform(image)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: File not found {img_path}. Returning dummy data.\")\n",
    "            image = torch.zeros((3, 224, 224)) # Match transform output shape\n",
    "            label = torch.zeros(NUM_LABELS) # Match label shape\n",
    "            return image, label\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error loading {img_path}: {e}. Returning dummy data.\")\n",
    "            image = torch.zeros((3, 224, 224))\n",
    "            label = torch.zeros(NUM_LABELS)\n",
    "            return image, label\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        return image, label\n",
    "\n",
    "train_indices = [i for i, p in enumerate(partition) if p == 0]\n",
    "val_indices = [i for i, p in enumerate(partition) if p == 1]\n",
    "test_indices = [i for i, p in enumerate(partition) if p == 2]\n",
    "\n",
    "train_dataset = CelebADataset([img_filenames[i] for i in train_indices], attr_labels[train_indices], IMG_DIR, transform)\n",
    "val_dataset = CelebADataset([img_filenames[i] for i in val_indices], attr_labels[val_indices], IMG_DIR, transform)\n",
    "test_dataset = CelebADataset([img_filenames[i] for i in test_indices], attr_labels[test_indices], IMG_DIR, transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "print(f\"Data loaded:\")\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980f0380",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ftrs = vit_model.classifier.in_features\n",
    "vit_model.classifier = nn.Linear(num_ftrs, NUM_LABELS)\n",
    "vit_model = vit_model.to(DEVICE)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "true_weight_decay = weight_decay if ENABLE_L2_REGULARIZATION else 0.0\n",
    "\n",
    "if RESET_ALL_PARAMETERS:\n",
    "    print(\"Resetting all model parameters to their initial state.\")\n",
    "    vit_model.apply(lambda m: m.reset_parameters() if hasattr(m, 'reset_parameters') else None)\n",
    "\n",
    "if ENABLE_VIB:\n",
    "    optimizer = optim.AdamW([\n",
    "        {'params': vit_model.vit_backbone.parameters(), 'lr': LEARNING_RATE_VIT, 'weight_decay': weight_decay},\n",
    "        {'params': vit_model.fc_mu.parameters(), 'lr': LEARNING_RATE_CLASSIFIER, 'weight_decay': weight_decay},\n",
    "        {'params': vit_model.fc_logvar.parameters(), 'lr': LEARNING_RATE_CLASSIFIER, 'weight_decay': weight_decay},\n",
    "        {'params': vit_model.classifier.parameters(), 'lr': LEARNING_RATE_CLASSIFIER, 'weight_decay': weight_decay}\n",
    "    ])\n",
    "else:\n",
    "    optimizer = optim.AdamW([\n",
    "        {'params': vit_model.vit.parameters(), 'lr': LEARNING_RATE_VIT, 'weight_decay': true_weight_decay},\n",
    "        {'params': vit_model.classifier.parameters(), 'lr': LEARNING_RATE_CLASSIFIER, 'weight_decay': true_weight_decay}\n",
    "    ])\n",
    "\n",
    "def label_smoothing_loss(outputs, targets, smoothing=-0.1):\n",
    "    num_classes = outputs.size(1)\n",
    "    # (1 - alpha) * L_CE(y, p) + alpha/C * L_CE(1, p)\n",
    "    loss = (1 - smoothing) * criterion(outputs, targets) + (smoothing / num_classes) * criterion(torch.ones_like(outputs), targets)\n",
    "    return loss\n",
    "\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_model_state = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1697233",
   "metadata": {},
   "source": [
    "### Step 4: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a10a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corrects_record = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    vit_model.train()\n",
    "    train_loss = 0.0\n",
    "    train_corrects = 0\n",
    "    train_total = 0\n",
    "\n",
    "    train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Train]\")\n",
    "    for inputs, labels_batch in train_pbar:\n",
    "        inputs, labels_batch = inputs.to(DEVICE), labels_batch.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        raw_outputs = vit_model(inputs, labels=labels_batch)\n",
    "        outputs = raw_outputs[\"logits\"]\n",
    "\n",
    "        if ENABLE_LABEL_SMOOTHING:\n",
    "            loss = label_smoothing_loss(outputs, labels_batch, LABEL_SMOOTHING_FACTOR)\n",
    "        else:\n",
    "            loss = criterion(outputs, labels_batch)\n",
    "\n",
    "        if ENABLE_L1_REGULARIZATION and L1_LAMBDA > 0:\n",
    "            l1_penalty = torch.tensor(0.0, device=DEVICE)\n",
    "            for param in vit_model.parameters():\n",
    "                if param.requires_grad:\n",
    "                    l1_penalty += torch.sum(torch.abs(param))\n",
    "            loss += L1_LAMBDA * l1_penalty\n",
    "\n",
    "        if ENABLE_VIB:\n",
    "            mu = raw_outputs[\"mu\"]\n",
    "            logvar = raw_outputs[\"logvar\"]\n",
    "            loss += raw_outputs[\"kld_loss\"] * beta\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        preds = torch.sigmoid(outputs) > 0.5\n",
    "        train_corrects += torch.sum(preds == labels_batch.byte()).item()\n",
    "        train_total += labels_batch.numel()\n",
    "        train_corrects_record.append(train_corrects)\n",
    "        if ENABLE_VIB:\n",
    "            train_pbar.set_postfix(\n",
    "                {\n",
    "                    \"loss\": loss.item(),\n",
    "                    \"acc\": train_corrects / train_total,\n",
    "                    \"mu\": mu.mean().item(),\n",
    "                    \"logvar\": logvar.mean().item(),\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            train_pbar.set_postfix({\"loss\": loss.item(), \"acc\": train_corrects / train_total})\n",
    "\n",
    "    epoch_train_loss = train_loss / len(train_loader.dataset)\n",
    "    epoch_train_acc = train_corrects / train_total\n",
    "\n",
    "    vit_model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_corrects = 0\n",
    "    val_total = 0\n",
    "\n",
    "    val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Val]\")\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels_batch in val_pbar:\n",
    "            inputs, labels_batch = inputs.to(DEVICE), labels_batch.to(DEVICE)\n",
    "\n",
    "            outputs = vit_model(inputs).logits\n",
    "\n",
    "        if ENABLE_LABEL_SMOOTHING:\n",
    "            loss = label_smoothing_loss(outputs, labels_batch, LABEL_SMOOTHING_FACTOR)\n",
    "        else:\n",
    "            loss = criterion(outputs, labels_batch)\n",
    "\n",
    "        if ENABLE_L1_REGULARIZATION and L1_LAMBDA > 0:\n",
    "            l1_penalty = torch.tensor(0.0, device=DEVICE)\n",
    "            for param in vit_model.parameters():\n",
    "                if param.requires_grad:\n",
    "                    l1_penalty += torch.sum(torch.abs(param))\n",
    "            loss += L1_LAMBDA * l1_penalty\n",
    "\n",
    "        if ENABLE_VIB:\n",
    "            mu = raw_outputs[\"mu\"]\n",
    "            logvar = raw_outputs[\"logvar\"]\n",
    "            loss += raw_outputs[\"kld_loss\"] * beta\n",
    "\n",
    "        val_loss += loss.item() * inputs.size(0)\n",
    "        preds = torch.sigmoid(outputs) > 0.5\n",
    "        val_corrects += torch.sum(preds == labels_batch.byte()).item()\n",
    "        val_total += labels_batch.numel()\n",
    "\n",
    "        val_pbar.set_postfix({\"loss\": loss.item()})\n",
    "        val_pbar.set_postfix({\"acc\": val_corrects / val_total})\n",
    "\n",
    "    epoch_val_loss = val_loss / len(val_loader.dataset)\n",
    "    epoch_val_acc = val_corrects / val_total\n",
    "    end_time = time.time()\n",
    "    epoch_duration = end_time - start_time\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | Duration: {epoch_duration:.2f}s\")\n",
    "    print(f\"  Train Loss: {epoch_train_loss:.4f} | Train Acc: {epoch_train_acc:.4f}\")\n",
    "    print(f\"  Val Loss:   {epoch_val_loss:.4f} | Val Acc:   {epoch_val_acc:.4f}\")\n",
    "\n",
    "    if epoch_val_loss < best_val_loss:\n",
    "        best_val_loss = epoch_val_loss\n",
    "        best_model_state = copy.deepcopy(vit_model.state_dict())\n",
    "        print(f\"  New best model saved with validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "if best_model_state:\n",
    "    vit_model.load_state_dict(best_model_state)\n",
    "    print(\"\\nLoaded best model weights based on validation loss.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7298a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStarting testing phase...\")\n",
    "vit_model.eval()\n",
    "test_loss = 0.0\n",
    "test_corrects = 0\n",
    "test_total = 0\n",
    "\n",
    "test_pbar = tqdm(test_loader, desc=\"Testing\")\n",
    "with torch.no_grad():\n",
    "    for inputs, labels_batch in test_pbar:\n",
    "        inputs, labels_batch = inputs.to(DEVICE), labels_batch.to(DEVICE)\n",
    "\n",
    "        outputs = vit_model(inputs)['logits']\n",
    "        loss = criterion(outputs, labels_batch)\n",
    "\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "        preds = torch.sigmoid(outputs) > 0.5\n",
    "        test_corrects += torch.sum(preds == labels_batch.byte()).item()\n",
    "        test_total += labels_batch.numel()\n",
    "\n",
    "        test_pbar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "final_test_loss = test_loss / len(test_loader.dataset)\n",
    "final_test_acc = test_corrects / test_total\n",
    "\n",
    "print(f\"\\nTest Results:\")\n",
    "print(f\"  Test Loss (without regularization): {final_test_loss:.4f}\")\n",
    "print(f\"  Test Accuracy: {final_test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff27d620",
   "metadata": {},
   "source": [
    "## Stage 2: Test inversion on the fine-tuned model\n",
    "\n",
    "### Step 1: Define inversion structure\n",
    "\n",
    "The structure is same as the `Self-Attention` network in the `invVIT-notebook.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0254c3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, bias=True, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features, bias=bias)\n",
    "        self.act = act_layer()\n",
    "        self.drop1 = nn.Dropout(drop)\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias)\n",
    "        self.drop2 = nn.Dropout(drop)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop2(x)\n",
    "        return x\n",
    "\n",
    "class InverseAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, 'dim should be divisible by num_heads'\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "        self.q = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.k = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.v = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "    def forward(self, x, context):\n",
    "        B, N, C = x.shape\n",
    "        B_ctx, N_ctx, C_ctx = context.shape\n",
    "        assert B == B_ctx and C == C_ctx\n",
    "        q = self.q(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "        k = self.k(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "        v = self.v(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        x_attn = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x_out = self.proj(x_attn)\n",
    "        x_out = self.proj_drop(x_out)\n",
    "        return x_out\n",
    "\n",
    "class InverseBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = InverseAttention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "    def forward(self, x, context):\n",
    "        x = x + self.attn(self.norm1(x), context)\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class PixelDecoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.patch_size = config.patch_size\n",
    "        self.num_channels = config.num_channels\n",
    "        self.image_size_h = config.image_size\n",
    "        self.image_size_w = config.image_size\n",
    "        self.num_patches = (self.image_size_h // self.patch_size) * (self.image_size_w // self.patch_size)\n",
    "        self.proj = nn.Linear(self.hidden_size, self.num_channels * self.patch_size * self.patch_size)\n",
    "        self.unpatchify = nn.Fold(output_size=(self.image_size_h, self.image_size_w),\n",
    "                                  kernel_size=(self.patch_size, self.patch_size),\n",
    "                                  stride=(self.patch_size, self.patch_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.shape[1] == self.num_patches + 1:\n",
    "            x = x[:, 1:, :]\n",
    "        elif x.shape[1] != self.num_patches:\n",
    "            raise ValueError(f\"Input embedding sequence length ({x.shape[1]}) doesn't match expected number of patches ({self.num_patches}) or patches+1.\")\n",
    "        x = self.proj(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        reconstructed_image = self.unpatchify(x)\n",
    "\n",
    "        return reconstructed_image\n",
    "\n",
    "\n",
    "class InverseViT(nn.Module):\n",
    "    def __init__(self, forward_model_config, norm_layer=nn.LayerNorm, act_layer=nn.GELU):\n",
    "        super().__init__()\n",
    "        self.config = forward_model_config # Store config for decoder\n",
    "        self.num_layers = forward_model_config.num_hidden_layers\n",
    "        self.hidden_dim = forward_model_config.hidden_size\n",
    "        self.num_heads = forward_model_config.num_attention_heads\n",
    "        self.mlp_ratio = getattr(forward_model_config, 'intermediate_size', self.hidden_dim * 4) / self.hidden_dim\n",
    "\n",
    "        self.norm_start = norm_layer(self.hidden_dim)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            InverseBlock(\n",
    "                dim=self.hidden_dim, num_heads=self.num_heads, mlp_ratio=self.mlp_ratio,\n",
    "                qkv_bias=True, attn_drop=0., drop=0., norm_layer=norm_layer, act_layer=act_layer\n",
    "            ) for _ in range(self.num_layers)])\n",
    "        self.norm_end = norm_layer(self.hidden_dim)\n",
    "        self.decoder = PixelDecoder(self.config)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None: nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward(self, final_features, intermediate_forward_outputs=None):\n",
    "        x = final_features\n",
    "        context = final_features\n",
    "        x = self.norm_start(x)\n",
    "\n",
    "        losses_q = []\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.blocks[i](x, context=context)\n",
    "\n",
    "        reconstructed_embedding = self.norm_end(x) # Shape: [B, N, D]\n",
    "        reconstructed_image = self.decoder(reconstructed_embedding) # Shape: [B, C, H, W]\n",
    "        return reconstructed_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692df4f4",
   "metadata": {},
   "source": [
    "Replace the path of inverse_model to the trained inverse model in `invViT-notebook.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f4f0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_model = InverseViT(vit_model.config)\n",
    "inverse_model.load_state_dict(torch.load(INVERSE_VIT_PATH))\n",
    "inverse_model.to(DEVICE)\n",
    "\n",
    "vit_model_original = ViTForImageClassification.from_pretrained(PRETRAINED_PATH).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f356686f",
   "metadata": {},
   "source": [
    "### Step 2: Test visualization\n",
    "\n",
    "Here we explore in detail the inverse effect of a single image.\n",
    "\n",
    "From the heatmap of the difference below, we can confirm that fine-tune model differs in the `last_hidden_state` numerically from the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f776451b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = Image.open('./TransformInverse/custom_test_imges/cat.jpg').convert('RGB')\n",
    "\n",
    "test_img_resized = test_img.resize((224, 224))\n",
    "test_img_transformed = transform(test_img_resized).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "original_features = vit_model_original.vit(test_img_transformed).last_hidden_state\n",
    "finetuned_features = vit_model.vit_backbone(test_img_transformed).last_hidden_state if ENABLE_VIB else vit_model.vit(test_img_transformed).last_hidden_state\n",
    "\n",
    "plt.figure(figsize=(10, 3), dpi=300)\n",
    "sns.heatmap(abs((original_features - finetuned_features)[0].cpu().detach().numpy()), cmap=\"Blues\", vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a7a0b6",
   "metadata": {},
   "source": [
    "### Step 3: Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994e95bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.array([0.5, 0.5, 0.5])\n",
    "std = np.array([0.5, 0.5, 0.5])\n",
    "\n",
    "reconstructed_original_img_tensor = inverse_model(original_features)\n",
    "reconstructed_original_img_np = reconstructed_original_img_tensor.squeeze(0).permute(1, 2, 0).cpu().detach().numpy()\n",
    "reconstructed_original_img_denorm = reconstructed_original_img_np * std + mean\n",
    "reconstructed_original_img_clipped = np.clip(reconstructed_original_img_denorm, 0, 1)\n",
    "\n",
    "reconstructed_finetuned_img_tensor = inverse_model(finetuned_features)\n",
    "reconstructed_finetuned_img_np = reconstructed_finetuned_img_tensor.squeeze(0).permute(1, 2, 0).cpu().detach().numpy()\n",
    "reconstructed_finetuned_img_denorm = reconstructed_finetuned_img_np * std + mean\n",
    "reconstructed_finetuned_img_clipped = np.clip(reconstructed_finetuned_img_denorm, 0, 1)\n",
    "\n",
    "plt.figure(figsize=(10, 3), dpi=300)\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(test_img_resized)\n",
    "plt.title(\"Input\")\n",
    "plt.axis('off')\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(reconstructed_original_img_clipped)\n",
    "plt.title(\"Reconstructed(Original)\")\n",
    "plt.axis('off')\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(reconstructed_finetuned_img_clipped)\n",
    "plt.title(\"Reconstructed(Fine-tuned)\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cd0204",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_img_np = np.array(test_img_resized).astype(np.float32) / 255.0\n",
    "\n",
    "mse_original = mean_squared_error(original_img_np, reconstructed_original_img_clipped)\n",
    "psnr_original = peak_signal_noise_ratio(original_img_np, reconstructed_original_img_clipped, data_range=1.0)\n",
    "ssim_original = structural_similarity(original_img_np, reconstructed_original_img_clipped, data_range=1.0, channel_axis=-1, win_size=7)\n",
    "\n",
    "mse_finetuned = mean_squared_error(original_img_np, reconstructed_finetuned_img_clipped)\n",
    "psnr_finetuned = peak_signal_noise_ratio(original_img_np, reconstructed_finetuned_img_clipped, data_range=1.0)\n",
    "ssim_finetuned = structural_similarity(original_img_np, reconstructed_finetuned_img_clipped, data_range=1.0, channel_axis=-1, win_size=7)\n",
    "\n",
    "\n",
    "print(\"--- Metrics vs Original Image ---\")\n",
    "print(\"\\nReconstruction (Original Model):\")\n",
    "print(f\"  MSE:  {mse_original:.4f}\")\n",
    "print(f\"  PSNR: {psnr_original:.2f} dB\")\n",
    "print(f\"  SSIM: {ssim_original:.4f}\")\n",
    "\n",
    "print(\"\\nReconstruction (Fine-tuned Model):\")\n",
    "print(f\"  MSE:  {mse_finetuned:.4f}\")\n",
    "print(f\"  PSNR: {psnr_finetuned:.2f} dB\")\n",
    "print(f\"  SSIM: {ssim_finetuned:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
