{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbd3e639",
   "metadata": {},
   "source": [
    "# Sample Code for training `invViT`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa43adf",
   "metadata": {},
   "source": [
    "### Step 1: Import Required Libraries and set configurations\n",
    "\n",
    "Preparations:\n",
    "\n",
    "- Dependencies in `requirements.txt`（`pip install -r requirements.txt`）\n",
    "- ViT-Large model (set the folder in the `model_name` variable)\n",
    "- OpenImages dataset (set the folder in the `dataset_path` variable, pure images should be in the folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ffeee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import lpips\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from transformers import ViTForImageClassification, ViTImageProcessor\n",
    "from collections import deque\n",
    "\n",
    "# --- Configuration (Adjust as needed) ---\n",
    "model_name = './TransformInverse'\n",
    "dataset_dir = './TransformInverse/dataset_OID2018'\n",
    "batch_size = 16 # Reduce if memory errors occur with decoder\n",
    "learning_rate_q = 1e-4\n",
    "learning_rate_q_prime = 1e-4 # May need adjustment for pixel reconstruction\n",
    "epochs_q = 5\n",
    "epochs_q_prime = 20 # Might need more epochs for pixel reconstruction\n",
    "num_workers = 2\n",
    "seed = 42\n",
    "val_split = 0.15\n",
    "test_split = 0.15\n",
    "print_every = 500   # Print visualization results every N batches while training\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "print(\"Loading forward model...\")\n",
    "try:\n",
    "    forward_model = ViTForImageClassification.from_pretrained(model_name)\n",
    "    processor = ViTImageProcessor.from_pretrained(model_name)\n",
    "    config = forward_model.config # Get config for decoder\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model/processor from {model_name}: {e}\")\n",
    "    exit()\n",
    "\n",
    "forward_model.to(device)\n",
    "forward_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c969c4",
   "metadata": {},
   "source": [
    "### Step 2: Define network architecture\n",
    "\n",
    "Self attention mode by default. Switch to cross attention by uncomment some of these lines:\n",
    "```python\n",
    "q = self.q(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "# q = self.q(context).reshape(B_ctx, N_ctx, self.num_heads, C_ctx // self.num_heads).permute(0, 2, 1, 3)\n",
    "k = self.k(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "# k = self.k(context).reshape(B_ctx, N_ctx, self.num_heads, C_ctx // self.num_heads).permute(0, 2, 1, 3)\n",
    "v = self.v(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "# v = self.v(context).reshape(B_ctx, N_ctx, self.num_heads, C_ctx // self.num_heads).permute(0, 2, 1, 3)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ec20a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, bias=True, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features, bias=bias)\n",
    "        self.act = act_layer()\n",
    "        self.drop1 = nn.Dropout(drop)\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias)\n",
    "        self.drop2 = nn.Dropout(drop)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop2(x)\n",
    "        return x\n",
    "\n",
    "class InverseAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, 'dim should be divisible by num_heads'\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "        self.q = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.k = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.v = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "    def forward(self, x, context):\n",
    "        B, N, C = x.shape\n",
    "        B_ctx, N_ctx, C_ctx = context.shape\n",
    "        assert B == B_ctx and C == C_ctx\n",
    "\n",
    "        q = self.q(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "        # q = self.q(context).reshape(B_ctx, N_ctx, self.num_heads, C_ctx // self.num_heads).permute(0, 2, 1, 3)\n",
    "        k = self.k(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "        # k = self.k(context).reshape(B_ctx, N_ctx, self.num_heads, C_ctx // self.num_heads).permute(0, 2, 1, 3)\n",
    "        v = self.v(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "        # v = self.v(context).reshape(B_ctx, N_ctx, self.num_heads, C_ctx // self.num_heads).permute(0, 2, 1, 3)\n",
    "        \n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        x_attn = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x_out = self.proj(x_attn)\n",
    "        x_out = self.proj_drop(x_out)\n",
    "        return x_out\n",
    "\n",
    "class InverseBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = InverseAttention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "    def forward(self, x, context):\n",
    "        x = x + self.attn(self.norm1(x), context)\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class PixelDecoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.patch_size = config.patch_size\n",
    "        self.num_channels = config.num_channels\n",
    "        self.image_size_h = config.image_size # Assuming square image from config.image_size\n",
    "        self.image_size_w = config.image_size\n",
    "        self.num_patches = (self.image_size_h // self.patch_size) * (self.image_size_w // self.patch_size)\n",
    "        self.proj = nn.Linear(self.hidden_size, self.num_channels * self.patch_size * self.patch_size)\n",
    "        self.unpatchify = nn.Fold(output_size=(self.image_size_h, self.image_size_w),\n",
    "                                  kernel_size=(self.patch_size, self.patch_size),\n",
    "                                  stride=(self.patch_size, self.patch_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Reconstructed embeddings. Shape [B, N, D].\n",
    "                              N = num_patches + 1 (if CLS token included).\n",
    "        \"\"\"\n",
    "        # Assume the first token is CLS if N = num_patches + 1, discard it\n",
    "        if x.shape[1] == self.num_patches + 1:\n",
    "            x = x[:, 1:, :] # Shape: [B, num_patches, D]\n",
    "        elif x.shape[1] != self.num_patches:\n",
    "            raise ValueError(f\"Input embedding sequence length ({x.shape[1]}) doesn't match expected number of patches ({self.num_patches}) or patches+1.\")\n",
    "\n",
    "        x = self.proj(x) # Shape: [B, num_patches, C * patch_size * patch_size]\n",
    "\n",
    "        # Reshape for Fold: Fold expects [B, C * kH * kW, L] where L is num_patches\n",
    "        x = x.transpose(1, 2) # Shape: [B, C * patch_size * patch_size, num_patches]\n",
    "\n",
    "        # Unpatchify\n",
    "        reconstructed_image = self.unpatchify(x) # Shape: [B, C, H, W]\n",
    "\n",
    "        return reconstructed_image\n",
    "\n",
    "\n",
    "class InverseViT(nn.Module):\n",
    "    \"\"\" Inverse Vision Transformer Network with Pixel Decoder. \"\"\"\n",
    "    def __init__(self, forward_model_config, norm_layer=nn.LayerNorm, act_layer=nn.GELU):\n",
    "        super().__init__()\n",
    "        self.config = forward_model_config # Store config for decoder\n",
    "        self.num_layers = forward_model_config.num_hidden_layers\n",
    "        self.hidden_dim = forward_model_config.hidden_size\n",
    "        self.num_heads = forward_model_config.num_attention_heads\n",
    "        self.mlp_ratio = getattr(forward_model_config, 'intermediate_size', self.hidden_dim * 4) / self.hidden_dim\n",
    "        self.norm_start = norm_layer(self.hidden_dim)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            InverseBlock(\n",
    "                dim=self.hidden_dim, num_heads=self.num_heads, mlp_ratio=self.mlp_ratio,\n",
    "                qkv_bias=True, attn_drop=0., drop=0., norm_layer=norm_layer, act_layer=act_layer\n",
    "            ) for _ in range(self.num_layers)])\n",
    "        self.norm_end = norm_layer(self.hidden_dim)\n",
    "        self.decoder = PixelDecoder(self.config)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None: nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward(self, final_features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            final_features (torch.Tensor): Output of the last encoder layer of the forward ViT. [B, N, D]\n",
    "            intermediate_forward_outputs (list[torch.Tensor], optional): Needed for 'train_q'.\n",
    "            mode (str): 'train_q', 'train_q_prime', or 'eval'.\n",
    "\n",
    "        Returns:\n",
    "            Reconstructed image tensor [B, C, H, W].\n",
    "        \"\"\"\n",
    "        x = final_features\n",
    "        context = final_features\n",
    "        x = self.norm_start(x)\n",
    "\n",
    "        losses_q = []\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.blocks[i](x, context=context)\n",
    "\n",
    "        # Output after blocks is the reconstructed embedding\n",
    "        reconstructed_embedding = self.norm_end(x) # Shape: [B, N, D]\n",
    "\n",
    "        reconstructed_image = self.decoder(reconstructed_embedding) # Shape: [B, C, H, W]\n",
    "        return reconstructed_image\n",
    "    \n",
    "class ReconstructionLoss(nn.Module):\n",
    "    def __init__(self, lambda_l1=1.0, lambda_perceptual=0.1):\n",
    "        super().__init__()\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.perceptual_loss = lpips.LPIPS(net='vgg')\n",
    "        self.lambda_l1 = lambda_l1\n",
    "        self.lambda_perceptual = lambda_perceptual\n",
    "        \n",
    "    def forward(self, reconstructed, original):\n",
    "        l1 = self.l1_loss(reconstructed, original)\n",
    "        mse = self.mse_loss(reconstructed, original)\n",
    "        perceptual = self.perceptual_loss(reconstructed, original).mean()\n",
    "        total_loss = l1 + mse + self.lambda_perceptual * perceptual\n",
    "        \n",
    "        return {\n",
    "            'total': total_loss,\n",
    "            'l1': l1,\n",
    "            'mse': mse,\n",
    "            'perceptual': perceptual\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7f5329",
   "metadata": {},
   "source": [
    "### Step 3: Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25b35db",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_forward_outputs(forward_model, inputs, device):\n",
    "    forward_model.eval()\n",
    "    inputs = inputs.to(device)\n",
    "    outputs = forward_model(inputs, output_hidden_states=True)\n",
    "    hidden_states = outputs.hidden_states\n",
    "    if not isinstance(hidden_states, tuple) or len(hidden_states) != forward_model.config.num_hidden_layers + 1:\n",
    "        raise ValueError(\"Failed to retrieve hidden states correctly.\")\n",
    "    original_embeddings = hidden_states[0].detach()\n",
    "    intermediate_and_final_outputs = tuple(h.detach() for h in hidden_states)\n",
    "    final_features = hidden_states[-1].detach()\n",
    "    return original_embeddings, intermediate_and_final_outputs, final_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1d290e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_height = config.image_size\n",
    "image_width = config.image_size\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_height, image_width)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=getattr(config, 'image_mean', [0.5, 0.5, 0.5]),\n",
    "                            std=getattr(config, 'image_std', [0.5, 0.5, 0.5]))\n",
    "])\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        supported_extensions = ('.png', '.jpg', '.jpeg', '.bmp', '.gif', '.tiff')\n",
    "        try:\n",
    "            self.image_files = [f for f in os.listdir(root_dir)\n",
    "                                if os.path.isfile(os.path.join(root_dir, f)) and f.lower().endswith(supported_extensions)]\n",
    "            if not self.image_files: print(f\"Warning: No images found in {root_dir}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Dataset directory not found: {root_dir}\"); self.image_files = []\n",
    "        self.transform = transform\n",
    "        print(f\"Found {len(self.image_files)} images in {root_dir}\")\n",
    "    def __len__(self): return len(self.image_files)\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.image_files): raise IndexError(\"Index out of bounds\")\n",
    "        img_name = os.path.join(self.root_dir, self.image_files[idx])\n",
    "        try:\n",
    "            image = Image.open(img_name).convert('RGB')\n",
    "            if self.transform: image = self.transform(image)\n",
    "            return image # Return the transformed image tensor\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error loading image {img_name}: {e}. Returning dummy.\")\n",
    "            # Ensure dummy has correct shape [C, H, W]\n",
    "            return torch.zeros((config.num_channels, image_height, image_width))\n",
    "\n",
    "dataset = ImageDataset(root_dir=dataset_dir, transform=transform)\n",
    "if len(dataset) == 0: raise ValueError(f\"Dataset empty. Check path: {dataset_dir}\")\n",
    "\n",
    "val_count = int(val_split * len(dataset)); test_count = int(test_split * len(dataset))\n",
    "train_count = len(dataset) - val_count - test_count\n",
    "if train_count <= 0 or val_count <= 0:\n",
    "        print(\"Warning: Dataset small, adjusting splits.\"); val_count = max(1, int(0.1 * len(dataset)))\n",
    "        train_count = len(dataset) - val_count; test_count = 0\n",
    "\n",
    "print(f\"Splits - Train: {train_count}, Val: {val_count}, Test: {test_count}\")\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset, [train_count, val_count, test_count], generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "# --- Initialize Inverse Model (includes PixelDecoder) ---\n",
    "print(\"Initializing inverse model with pixel decoder...\")\n",
    "inverse_model = InverseViT(config) # Pass config now\n",
    "inverse_model.to(device)\n",
    "print(f\"Inverse model params: {sum(p.numel() for p in inverse_model.parameters() if p.requires_grad)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a07a2d7",
   "metadata": {},
   "source": [
    "### Step 4: Helper functions for training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e398865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(tensor, mean, std):\n",
    "    \"\"\"Denormalizes a tensor image with mean and standard deviation.\n",
    "    Args:\n",
    "        tensor (torch.Tensor): Tensor image of size (N, C, H, W) to be denormalized.\n",
    "        mean (list or tuple): Mean values for each channel.\n",
    "        std (list or tuple): Standard deviation values for each channel.\n",
    "    Returns:\n",
    "        torch.Tensor: Denormalized tensor image.\n",
    "    \"\"\"\n",
    "    mean = torch.tensor(mean, device=tensor.device).view(1, -1, 1, 1)\n",
    "    std = torch.tensor(std, device=tensor.device).view(1, -1, 1, 1)\n",
    "    return tensor * std + mean\n",
    "\n",
    "def visualize_reconstruction(\n",
    "    forward_model,\n",
    "    inverse_model,\n",
    "    test_loader,\n",
    "    device,\n",
    "    pixel_loss_history,\n",
    "    current_batch_idx,\n",
    "    mean,\n",
    "    std,\n",
    "    num_images=5,\n",
    "    loss_history_length=1000\n",
    "    ):\n",
    "    print(f\"Generating visualization for batch {current_batch_idx}...\")\n",
    "\n",
    "    # --- Store original modes and set to eval ---\n",
    "    inv_mode_orig = inverse_model.training\n",
    "    fwd_mode_orig = forward_model.training\n",
    "    inverse_model.eval()\n",
    "    forward_model.eval()\n",
    "\n",
    "    try:\n",
    "        try:\n",
    "            test_images = next(iter(test_loader))\n",
    "            if isinstance(test_images, (list, tuple)):\n",
    "                 test_images = test_images[0] # If dataset returns (image, label)\n",
    "\n",
    "            if test_images.shape[0] < num_images:\n",
    "                 print(f\"Warning: Test batch size ({test_images.shape[0]}) is smaller than num_images ({num_images}).\")\n",
    "                 num_images = test_images.shape[0]\n",
    "\n",
    "            test_images = test_images[:num_images].to(device) # Select first N images\n",
    "\n",
    "        except StopIteration:\n",
    "            print(\"Error: Cannot get data from test_loader\")\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting data from test_loader: {e}\")\n",
    "            return\n",
    "\n",
    "        with torch.no_grad():\n",
    "            _, _, final_features = get_forward_outputs(forward_model, test_images, device)\n",
    "            reconstructed_images = inverse_model(final_features)\n",
    "\n",
    "        original_images_denorm = denormalize(test_images, mean, std)\n",
    "        reconstructed_images_denorm = denormalize(reconstructed_images, mean, std)\n",
    "\n",
    "        original_images_np = original_images_denorm.clamp(0, 1).cpu().numpy()\n",
    "        reconstructed_images_np = reconstructed_images_denorm.clamp(0, 1).cpu().numpy()\n",
    "\n",
    "        # Transpose from (N, C, H, W) to (N, H, W, C) for matplotlib\n",
    "        if original_images_np.shape[1] == 3: # RGB\n",
    "            original_images_np = np.transpose(original_images_np, (0, 2, 3, 1))\n",
    "            reconstructed_images_np = np.transpose(reconstructed_images_np, (0, 2, 3, 1))\n",
    "        elif original_images_np.shape[1] == 1: # Grayscale\n",
    "            original_images_np = np.squeeze(original_images_np, axis=1)\n",
    "            reconstructed_images_np = np.squeeze(reconstructed_images_np, axis=1)\n",
    "\n",
    "        fig = plt.figure(figsize=(max(15, num_images * 3), 11)) # Adjust width based on num_images\n",
    "        gs = gridspec.GridSpec(3, num_images, height_ratios=[1, 1, 1.5], wspace=0.1, hspace=0.3)\n",
    "\n",
    "        for i in range(num_images):\n",
    "            ax_orig = fig.add_subplot(gs[0, i])\n",
    "            ax_orig.imshow(original_images_np[i], cmap='gray' if len(original_images_np[i].shape) == 2 else None)\n",
    "            ax_orig.set_title(f'Original {i+1}')\n",
    "            ax_orig.axis('off')\n",
    "\n",
    "            ax_rec = fig.add_subplot(gs[1, i])\n",
    "            ax_rec.imshow(reconstructed_images_np[i], cmap='gray' if len(reconstructed_images_np[i].shape) == 2 else None)\n",
    "            ax_rec.set_title(f'Reconstructed {i+1}')\n",
    "            ax_rec.axis('off')\n",
    "\n",
    "        ax_loss = fig.add_subplot(gs[2, :]) # Span across the bottom row\n",
    "\n",
    "        loss_list = list(pixel_loss_history)\n",
    "        num_losses = len(loss_list)\n",
    "\n",
    "        if num_losses > 0:\n",
    "             start_batch_idx = max(0, current_batch_idx - num_losses + 1)\n",
    "             batch_indices = list(range(start_batch_idx, current_batch_idx + 1))\n",
    "\n",
    "             if len(batch_indices) != num_losses:\n",
    "                  print(f\"Warning: Adjusting batch indices for plotting. Current batch: {current_batch_idx}, Losses count: {num_losses}\")\n",
    "                  batch_indices = list(range(max(0, current_batch_idx - num_losses + 1), current_batch_idx + 1))\n",
    "                  if len(batch_indices) != num_losses:\n",
    "                      batch_indices = list(range(num_losses))\n",
    "\n",
    "\n",
    "             ax_loss.plot(batch_indices, loss_list, label='Loss', color='orange')\n",
    "             ax_loss.set_xlabel('Batch Index')\n",
    "             ax_loss.set_ylabel('Loss')\n",
    "             ax_loss.set_title(f'Recent Pixel Loss History (Last {min(num_losses, loss_history_length)} Batches up to Batch {current_batch_idx})')\n",
    "             ax_loss.legend()\n",
    "             ax_loss.grid(True, alpha = 0.4)\n",
    "        else:\n",
    "            ax_loss.set_title('Loss History (No batch data yet)')\n",
    "            ax_loss.text(0.5, 0.5, 'Waiting for training data...', ha='center', va='center', transform=ax_loss.transAxes)\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout\n",
    "        fig.suptitle(f'Reconstruction Visualization at Batch {current_batch_idx} ({time.strftime(\"%Y-%m-%d %H:%M:%S\")})', fontsize=16)\n",
    "        plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during visualization: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    finally:\n",
    "        inverse_model.train(inv_mode_orig)\n",
    "        forward_model.train(fwd_mode_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adf022d",
   "metadata": {},
   "source": [
    "### Step 5: Training loop\n",
    "\n",
    "uncomment the following block to recover weights from a previous run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0495dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse_model.load_state_dict(torch.load('TransformInverse/model_0414b.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2ef5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn_pixel = ReconstructionLoss(lambda_l1=1.0, lambda_perceptual=0.15).to(device)  # For Phase 2\n",
    "\n",
    "optimizer = optim.AdamW(inverse_model.parameters(), lr=learning_rate_q_prime)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\", factor=0.2, patience=3, verbose=True)\n",
    "loss_records = deque(maxlen=1000)\n",
    "\n",
    "for epoch in range(epochs_q_prime):\n",
    "    epoch_start_time = time.time()\n",
    "    inverse_model.train()\n",
    "    total_train_loss_q_prime = 0.0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs_q_prime}\", leave=False)\n",
    "    # Training\n",
    "    for batch_idx, original_images in enumerate(progress_bar):\n",
    "        original_images = original_images.to(device)\n",
    "\n",
    "        _, _, final_features = get_forward_outputs(forward_model, original_images, device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        reconstructed_images = inverse_model(final_features)\n",
    "\n",
    "        loss = loss_fn_pixel(reconstructed_images, original_images)['total']\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss_q_prime += loss.item()\n",
    "        progress_bar.set_postfix(\n",
    "            {\"Loss(pixel)\": f\"{loss.item():.4f}\", \"LR\": f\"{optimizer.param_groups[0]['lr']:.1e}\"}\n",
    "        )\n",
    "        loss_records.append(loss.item())\n",
    "        if batch_idx % print_every == 0:\n",
    "            visualize_reconstruction(\n",
    "                forward_model=forward_model,\n",
    "                inverse_model=inverse_model,\n",
    "                test_loader=test_loader,\n",
    "                device=device,\n",
    "                pixel_loss_history=loss_records,\n",
    "                current_batch_idx=batch_idx,\n",
    "                mean=[0.5, 0.5, 0.5],\n",
    "                std=[0.5, 0.5, 0.5],\n",
    "                num_images=5,\n",
    "                loss_history_length=1000,\n",
    "            )\n",
    "\n",
    "    avg_train_loss_q_prime = total_train_loss_q_prime / len(train_loader) if len(train_loader) > 0 else 0\n",
    "\n",
    "    # Validation\n",
    "    inverse_model.eval()\n",
    "    total_val_loss_q_prime = 0.0\n",
    "    val_progress_bar = tqdm(val_loader, desc=f\"Validation\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for original_images in val_progress_bar:\n",
    "            original_images = original_images.to(device)\n",
    "            _, _, final_features = get_forward_outputs(forward_model, original_images, device)\n",
    "            reconstructed_images = inverse_model(final_features)\n",
    "            # Use pixel loss for validation\n",
    "            total_val_loss_q_prime += loss_fn_pixel(reconstructed_images, original_images)['total'].item()\n",
    "\n",
    "    avg_val_loss_q_prime = total_val_loss_q_prime / len(val_loader) if len(val_loader) > 0 else 0\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{epochs_q_prime} - Time: {epoch_time:.2f}s - Train Loss: {avg_train_loss_q_prime:.4f} - Val Loss: {avg_val_loss_q_prime:.4f}\"\n",
    "    )\n",
    "    scheduler.step(avg_val_loss_q_prime)\n",
    "\n",
    "print(\"\\nTraining finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23afb63",
   "metadata": {},
   "source": [
    "### Step 6: Save weights or test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bef9fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(inverse_model.state_dict(), 'TransformInverse/model_0414c(Blackbox-Self).pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a0fae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from skimage.metrics import peak_signal_noise_ratio as compare_psnr\n",
    "from skimage.metrics import structural_similarity as compare_ssim\n",
    "\n",
    "inverse_model.eval()\n",
    "forward_model.eval()\n",
    "\n",
    "mse_list = []\n",
    "mae_list = []\n",
    "psnr_list = []\n",
    "ssim_list = []\n",
    "\n",
    "\n",
    "mean = torch.tensor(getattr(config, 'image_mean', [0.5, 0.5, 0.5]), device=device).view(1, -1, 1, 1)\n",
    "std = torch.tensor(getattr(config, 'image_std', [0.5, 0.5, 0.5]), device=device).view(1, -1, 1, 1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Evaluating on test set\"):\n",
    "        original_images = batch.to(device)\n",
    "        _, _, final_features = get_forward_outputs(forward_model, original_images, device)\n",
    "        reconstructed_images = inverse_model(final_features)\n",
    "\n",
    "        # Denormalize\n",
    "        original_images_denorm = original_images * std + mean\n",
    "        reconstructed_images_denorm = reconstructed_images * std + mean\n",
    "\n",
    "        original_images_denorm = original_images_denorm.clamp(0, 1).cpu().numpy()\n",
    "        reconstructed_images_denorm = reconstructed_images_denorm.clamp(0, 1).cpu().numpy()\n",
    "\n",
    "        for orig, rec in zip(original_images_denorm, reconstructed_images_denorm):\n",
    "            # (C, H, W) -> (H, W, C)\n",
    "            orig_img = np.transpose(orig, (1, 2, 0))\n",
    "            rec_img = np.transpose(rec, (1, 2, 0))\n",
    "\n",
    "            mse = mean_squared_error(orig_img.flatten(), rec_img.flatten())\n",
    "            mae = mean_absolute_error(orig_img.flatten(), rec_img.flatten())\n",
    "            psnr = compare_psnr(orig_img, rec_img, data_range=1.0)\n",
    "            ssim = compare_ssim(orig_img, rec_img, multichannel=True, data_range=1.0)\n",
    "\n",
    "            mse_list.append(mse)\n",
    "            mae_list.append(mae)\n",
    "            psnr_list.append(psnr)\n",
    "            ssim_list.append(ssim)\n",
    "\n",
    "print(f\"Test set results:\")\n",
    "print(f\"  MSE:  {np.mean(mse_list):.6f}\")\n",
    "print(f\"  MAE:  {np.mean(mae_list):.6f}\")\n",
    "print(f\"  PSNR: {np.mean(psnr_list):.2f}\")\n",
    "print(f\"  SSIM: {np.mean(ssim_list):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
